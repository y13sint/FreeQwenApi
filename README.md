# API-прокси для Qwen AI

Локальный API-прокси сервер для работы с Qwen AI через браузерную эмуляцию. Позволяет использовать модели Qwen без официального API-ключа.

- **Бесплатный доступ**: Используйте модели Qwen без оплаты API-ключа
- **Полная совместимость**: Поддержка OpenAI-совместимого интерфейса для простой интеграции

## Оглавление

- [Назначение](#назначение)
- [Установка](#установка)
- [Запуск](#запуск)
- [API Reference](#api-reference)
  - [Основные эндпоинты](#основные-эндпоинты)
  - [Форматы запросов](#форматы-запросов)
  - [Работа с изображениями](#работа-с-изображениями)
  - [Управление диалогами](#управление-диалогами)
  - [Загрузка файлов](#загрузка-файлов)
- [Примеры использования](#примеры-использования)
  - [Текстовые запросы](#текстовые-запросы)
  - [Запросы с изображениями](#запросы-с-изображениями)
  - [Запросы через OpenAI SDK](#запросы-через-openai-sdk)
- [Работа с контекстом](#работа-с-контекстом)
- [Совместимость с OpenAI API](#совместимость-с-openai-api)
  - [Поддержка streaming режима](#поддержка-streaming-режима)
  - [Примеры использования с OpenAI SDK](#примеры-использования-с-openai-sdk)
  - [Ограничения совместимости](#ограничения-совместимости)
- [Особенности реализации](#особенности-реализации)

## Назначение

Этот проект позволяет:

- Использовать модели Qwen AI через локальный API
- Сохранять контекст диалогов между запросами
- Управлять диалогами через API
- Выбирать различные модели Qwen для генерации ответов
- Отправлять изображения для анализа моделью
- Использовать OpenAI-совместимый API с поддержкой streaming режима

## Установка

1. Клонировать репозиторий
2. Установить зависимости:

```bash
npm install
```

## Запуск

```bash
npm start
```

Также доступен файл быстрого запуска:

```
start.bat
```

При первом запуске откроется окно браузера, в котором необходимо авторизоваться на сайте Qwen AI. После успешной авторизации нажмите Enter в консоли для продолжения.

## API Reference

### Основные эндпоинты

| Эндпоинт | Метод | Описание |
|----------|-------|----------|
| `/api/chat` | POST | Отправка сообщения и получение ответа |
| `/api/chat/completions` | POST | OpenAI-совместимый эндпоинт с поддержкой streaming |
| `/api/models` | GET | Получение списка доступных моделей |
| `/api/status` | GET | Проверка статуса авторизации |
| `/api/files/upload` | POST | Загрузка изображения для использования в запросах |
| `/api/chats` | POST | Создание нового диалога |
| `/api/chats` | GET | Получение списка всех диалогов |
| `/api/chats/:chatId` | GET | Получение истории диалога |
| `/api/chats/:chatId` | DELETE | Удаление диалога |
| `/api/chats/:chatId/rename` | PUT | Переименование диалога |
| `/api/chats/cleanup` | POST | Автоудаление диалогов по критериям |

### Форматы запросов

Прокси поддерживает два формата запросов к `/api/chat`:

#### 1. Упрощенный формат с параметром `message`

```json
{
  "message": "Текст сообщения",
  "model": "qwen-max-latest",
  "chatId": "идентификатор_чата"
}
```

#### 2. Формат, совместимый с официальным API Qwen, с параметром `messages`

```json
{
  "messages": [
    {"role": "user", "content": "Привет, как дела?"}
  ],
  "model": "qwen-max-latest",
  "chatId": "идентификатор_чата"
}
```

### Работа с историей диалога

**Важно понимать:** Прокси использует внутреннюю систему хранения истории диалогов на сервере.

1. При использовании формата `message` - сообщение просто добавляется в историю диалога.
2. При использовании формата `messages` - из массива извлекается только последнее сообщение пользователя и добавляется в историю.

При отправке запроса к официальному API Qwen **всегда** используется полная история диалога, связанная с указанным `chatId`. Это означает, что при использовании параметра `messages` вам достаточно включить только новое сообщение пользователя с ролью "user", а не всю историю диалога.

### Работа с изображениями

Прокси поддерживает отправку сообщений с изображениями в обоих форматах:

#### Формат `message` с изображением

```json
{
  "message": [
    {
      "type": "text",
      "text": "Опишите объекты на этом изображении"
    },
    {
      "type": "image",
      "image": "URL_ИЗОБРАЖЕНИЯ"
    }
  ],
  "model": "qwen3-235b-a22b",
  "chatId": "идентификатор_чата"
}
```

#### Формат `messages` с изображением

```json
{
  "messages": [
    {
      "role": "user", 
      "content": [
        {
          "type": "text",
          "text": "Опишите объекты на этом изображении"
        },
        {
          "type": "image",
          "image": "URL_ИЗОБРАЖЕНИЯ"
        }
      ]
    }
  ],
  "model": "qwen3-235b-a22b",
  "chatId": "идентификатор_чата"
}
```

### Получение URL изображения из интерфейса Qwen


Для отправки изображений через API прокси необходимо сначала получить URL изображения. Это можно сделать следующим образом:

#### Способ 1: Загрузка через API прокси

Отправьте POST запрос на эндпоинт `/api/files/upload` для загрузки изображения:

```bash
curl -X POST http://localhost:3264/api/files/upload \
  -F "file=@/путь/к/изображению.jpg"
```

Ответ будет содержать URL изображения для использования в запросах к API:

```json
{
  "imageUrl": "https://cdn.qwenlm.ai/user-id/file-id_filename.jpg?key=..."
}
```

#### Способ 2: Получение URL через веб-интерфейс Qwen

1. Загрузите изображение в официальном веб-интерфейсе Qwen (<https://chat.qwen.ai/>)
2. Откройте инструменты разработчика в браузере (F12 или Ctrl+Shift+I)
3. Перейдите на вкладку "Network" (Сеть)
4. Найдите запрос к API Qwen, содержащий ваше изображение (обычно это запрос GetsToken)
5. В теле запроса найдите URL изображения, который выглядит примерно так: `https://cdn.qwenlm.ai/user-id/file-id_filename.jpg?key=...`
6. Скопируйте этот URL для использования в вашем API-запросе

### Управление диалогами

#### Создание нового диалога

```
POST http://localhost:3264/api/chats
```

Тело запроса:

```json
{
  "name": "Название диалога"
}
```

Возвращает:

```json
{
  "chatId": "уникальный_идентификатор"
}
```

#### Получение списка всех диалогов

```
GET http://localhost:3264/api/chats
```

#### Получение истории диалога

```
GET http://localhost:3264/api/chats/:chatId
```

#### Удаление диалога

```
DELETE http://localhost:3264/api/chats/:chatId
```

#### Переименование диалога

```
PUT http://localhost:3264/api/chats/:chatId/rename
```

Тело запроса:

```json
{
  "name": "Новое название чата"
}
```

#### Автоматическое удаление диалогов

```
POST http://localhost:3264/api/chats/cleanup
```

Тело запроса (все параметры опциональны):

```json
{
  "olderThan": 604800000, // Удалить чаты старше указанного времени (в мс), например 7 дней
  "userMessageCountLessThan": 3, // Удалить чаты с менее чем 3 сообщениями от пользователя
  "messageCountLessThan": 5, // Удалить чаты с менее чем 5 сообщениями всего
  "maxChats": 50 // Оставить только 50 самых новых чатов'
}
```

### Загрузка файлов

#### Загрузка изображения

```
POST http://localhost:3264/api/files/upload
```

Формат запроса: `multipart/form-data`

Параметры:
- `file` - файл изображения (поддерживаются форматы: jpg, jpeg, png, gif, webp)

Пример использования с curl:

```bash
curl -X POST http://localhost:3264/api/files/upload \
  -F "file=@/путь/к/изображению.jpg"
```

#### Пример использования через Postman

1. **Загрузка изображения**:
   - Создайте новый запрос POST к `http://localhost:3264/api/files/upload`
   - Выберите вкладку "Body"
   - Выберите тип "form-data"
   - Добавьте ключ "file" и выберите тип "File"
   - Загрузите изображение, нажав на кнопку "Select Files"
   - Нажмите "Send"

   Ответ будет содержать URL изображения:
   ```json
   {
     "imageUrl": "https://cdn.qwenlm.ai/user-id/file-id_filename.jpg?key=..."
   }
   ```

2. **Использование изображения в запросе**:
   - Создайте новый запрос POST к `http://localhost:3264/api/chat`
   - Выберите вкладку "Body"
   - Выберите тип "raw" и формат "JSON"
   - Вставьте следующий JSON, заменив `URL_ИЗОБРАЖЕНИЯ` на полученный URL:
   ```json
   {
     "message": [
       {
         "type": "text",
         "text": "Опишите объекты на этом изображении"
       },
       {
         "type": "image",
         "image": "URL_ИЗОБРАЖЕНИЯ"
       }
     ],
     "model": "qwen3-235b-a22b"
   }
   ```
   - Нажмите "Send"

Пример ответа:

```json
{
  "imageUrl": "https://cdn.qwenlm.ai/user-id/file-id_filename.jpg?key=..."
}
```

Полученный URL изображения можно использовать в запросах с изображениями, как описано в разделе [Работа с изображениями](#работа-с-изображениями).

## Примеры использования

### Текстовые запросы

#### Пример простого текстового запроса

```bash
curl -X POST http://localhost:3264/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Что такое искусственный интеллект?",
    "model": "qwen-max-latest"
  }'
```

#### Пример запроса в формате официального API

```bash
curl -X POST http://localhost:3264/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Что такое искусственный интеллект?"}
    ],
    "model": "qwen-max-latest"
  }'
```

### Запросы с изображениями

#### Пример загрузки изображения и отправки запроса с ним

```bash
# Шаг 1: Загрузка изображения
UPLOAD_RESPONSE=$(curl -s -X POST http://localhost:3264/api/files/upload \
  -F "file=@/путь/к/изображению.jpg")

# Шаг 2: Извлечение URL изображения
IMAGE_URL=$(echo $UPLOAD_RESPONSE | grep -o '"imageUrl":"[^"]*"' | sed 's/"imageUrl":"//;s/"//')

# Шаг 3: Отправка запроса с изображением
curl -X POST http://localhost:3264/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": [
      {
        "type": "text",
        "text": "Опишите объекты на этом изображении"
      },
      {
        "type": "image",
        "image": "'$IMAGE_URL'"
      }
    ],
    "model": "qwen3-235b-a22b"
  }'
```

#### Пример запроса с изображением

```bash
curl -X POST http://localhost:3264/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": [
      {
        "type": "text",
        "text": "Опишите объекты на этом изображении"
      },
      {
        "type": "image",
        "image": "https://cdn.qwenlm.ai/9e288181-c6b9-466d-9a68-afebbf31aa0a/b5e854e3-7286-4434-8ac7-aee0304d04d0_magway2-2.jpg?key=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJyZXNvdXJjZV91c2VyX2lkIjoiOWUyODgxODEtYzZiOS00NjZkLTlhNjgtYWZlYmJmMzFhYTBhIiwicmVzb3VyY2VfaWQiOiJiNWU4NTRlMy03Mjg2LTQ0MzQtOGFjNy1hZWUwMzA0ZDA0ZDAiLCJyZXNvdXJjZV9jaGF0X2lkIjpudWxsfQ.3aWPB5QZ6D5whL5cr6owNEeQoiL5NzgekSla8lNwSJk"
      }
    ],
    "model": "qwen3-235b-a22b"
  }'
```

### Запросы через OpenAI SDK

В директории `examples/openai-sdk` находятся примеры использования прокси через официальный OpenAI SDK:

```bash
# Установка зависимостей
npm install

npm run example:stream   # Пример с потоковым режимом

```

## Работа с контекстом

Система автоматически сохраняет историю диалога и отправляет ее в каждом запросе к API Qwen. Это позволяет моделям учитывать предыдущие сообщения при генерации ответов.

Для использования контекста в запросах:

1. Отправьте первое сообщение без указания `chatId`
2. Сохраните полученный `chatId` из ответа
3. Используйте этот `chatId` в последующих запросах

Пример последовательности запросов:

1. Первый запрос:

```json
{
  "message": "Привет, как тебя зовут?"
}
```

2. Ответ (сокращенно):

```json
{
  "chatId": "abcd-1234-5678",
  "choices": [...]
}
```

3. Второй запрос (с контекстом):

```json
{
  "message": "Сколько будет 2+2?",
  "chatId": "abcd-1234-5678"
}
```

## Совместимость с OpenAI API

Прокси поддерживает эндпоинт, совместимый с OpenAI API для подключения клиентов, которые работают с OpenAI API.

### Эндпоинт совместимости с OpenAI

| Эндпоинт | Метод | Описание |
|----------|-------|----------|
| `/api/chat/completions` | POST | Эндпоинт, совместимый с форматом OpenAI API |

### Особенности работы с эндпоинтом `/chat/completions`

Эндпоинт `/api/chat/completions` имеет следующие особенности:

1. **Создание нового чата для каждого запроса:** Каждый запрос к `/chat/completions` создаёт новый чат в системе с именем "OpenAI API Chat", что позволяет изолировать контексты разных запросов.

2. **Сохранение полной истории сообщений:** Все сообщения из запроса (включая системные, пользовательские и сообщения ассистента) сохраняются в истории чата. Это обеспечивает полную поддержку специальных инструкций через системные сообщения и прозрачную работу OpenAI SDK.

3. **Поддержка системных сообщений:** Прокси корректно обрабатывает и сохраняет системные сообщения (`role: "system"`), которые часто используются для настройки поведения модели.

4. **Полная совместимость с инструментами и библиотеками:** Такой подход обеспечивает максимальную совместимость с различными клиентскими библиотеками, которые используют OpenAI API.

Пример запроса с системным сообщением:

```json
{
  "messages": [
    {"role": "system", "content": "Ты эксперт по JavaScript. Отвечай только на вопросы о JavaScript."},
    {"role": "user", "content": "Как создать класс в JavaScript?"}
  ],
  "model": "qwen-max-latest"
}
```

### Поддержка streaming режима

Прокси поддерживает режим потоковой передачи ответов (streaming), что позволяет получать ответы по частям в режиме реального времени. Для использования streaming режима, добавьте параметр `stream: true` в запрос:

```json
{
  "messages": [
    {"role": "user", "content": "Напиши длинный рассказ о космосе"}
  ],
  "model": "qwen-max-latest",
  "stream": true
}
```

При использовании streaming режима, ответ будет возвращаться постепенно в формате Server-Sent Events (SSE), совместимом с OpenAI API.

### Примеры использования с OpenAI SDK

```javascript
// Пример использования с OpenAI Node.js SDK
import OpenAI from 'openai';
import fs from 'fs';
import axios from 'axios';

const openai = new OpenAI({
  baseURL: 'http://localhost:3264/api', // Базовый URL прокси
  apiKey: 'dummy-key', // Не требуется реальный ключ, но поле обязательное для библиотеки
});

// Запрос без streaming
const completion = await openai.chat.completions.create({
  messages: [{ role: 'user', content: 'Привет, как дела?' }],
  model: 'qwen-max-latest', // Используемая модель Qwen
});

console.log(completion.choices[0].message);

// Запрос со streaming
const stream = await openai.chat.completions.create({
  messages: [{ role: 'user', content: 'Расскажи длинную историю о космосе' }],
  model: 'qwen-max-latest',
  stream: true,
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || '');
}

// Загрузка и использование изображения
async function uploadAndAnalyzeImage(imagePath) {
  // Загрузка изображения через API прокси
  const formData = new FormData();
  formData.append('file', fs.createReadStream(imagePath));
  
  const uploadResponse = await axios.post('http://localhost:3264/api/files/upload', formData, {
    headers: { 'Content-Type': 'multipart/form-data' }
  });
  
  const imageUrl = uploadResponse.data.imageUrl;
  
  // Создание запроса с изображением
  const completion = await openai.chat.completions.create({
    messages: [
      { 
        role: 'user', 
        content: [
          { type: 'text', text: 'Опиши, что изображено на этой картинке?' },
          { type: 'image', image: imageUrl }
        ] 
      }
    ],
    model: 'qwen3-235b-a22b',
  });
  
  console.log(completion.choices[0].message.content);
}

// Использование: uploadAndAnalyzeImage('./image.jpg');
```

#### Пример использования через Postman с OpenAI-совместимым эндпоинтом

1. **Загрузка изображения**:
   - Создайте новый запрос POST к `http://localhost:3264/api/files/upload`
   - Выберите вкладку "Body"
   - Выберите тип "form-data"
   - Добавьте ключ "file" и выберите тип "File"
   - Загрузите изображение, нажав на кнопку "Select Files"
   - Нажмите "Send"
   - Скопируйте полученный URL изображения

2. **Запрос через OpenAI-совместимый эндпоинт**:
   - Создайте новый запрос POST к `http://localhost:3264/api/chat/completions`
   - Выберите вкладку "Body"
   - Выберите тип "raw" и формат "JSON"
   - Вставьте следующий JSON, заменив `URL_ИЗОБРАЖЕНИЯ` на полученный URL:
   ```json
   {
     "messages": [
       {
         "role": "user",
         "content": [
           {
             "type": "text",
             "text": "Опиши, что изображено на этой картинке?"
           },
           {
             "type": "image",
             "image": "URL_ИЗОБРАЖЕНИЯ"
           }
         ]
       }
     ],
     "model": "qwen3-235b-a22b"
   }
   ```
   - Нажмите "Send"

3. **Запрос с потоковым режимом (streaming)**:
   - Используйте тот же URL и тело запроса, но добавьте параметр `"stream": true`
   - Примечание: для корректного отображения потока в Postman, проверьте опцию "Preserve log" в консоли

### Ограничения совместимости

1. Некоторые специфичные для OpenAI параметры (например, `logprobs`, `functions` и т.д.) не поддерживаются, так как они отсутствуют в API Qwen.
2. Скорость потоковой передачи может отличаться от оригинального OpenAI API.


## Особенности реализации

- Прокси эмулирует взаимодействие с веб-интерфейсом Qwen через headless браузер
- Автоматическое управление сессиями и авторизацией
- Оптимизация производительности через пул браузерных страниц
- Возможность автоматического сохранения и восстановления состояния авторизации
